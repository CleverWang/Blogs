# 2.神经网络的数学基础

## 2.1初识神经网络

1. 在机器学习中，分类问题中的某个类别叫作**类（class）**。数据点叫作**样本（sample）**，某个样本对应的类叫作**标签（label）**
2. **训练集（training set）**，**测试集（test set）**
3. 针对MNIST，将训练数据（train_images 和 train_labels）输入神经网络；其次， 网络学习将图像和标签关联在一起；最后，网络对 test_images 生成预测，而我们将验证这些预测与 test_labels 中的标签是否匹配 
4. 神经网络的核心组件是**层（layer）**，它是一种数据处理模块，你可以将它看成数据过滤器。大多数深度学习都是将简单的层链接起来，从而实现渐进式的**数据蒸馏（data distillation）**
5. **编译（compile）**步骤：
   - **损失函数（loss function）**：网络如何衡量在训练数据上的性能，即网络如何朝着正确的方向前进
   - **优化器（optimizer）**：基于训练数据和损失函数来更新网络的机制 
   - **在训练和测试过程中需要监控的指标（metric）**
6. 在训练数据上**拟合（fit）**模型 
7. 检查一下模型在测试集上的**性能** 
8. **过拟合**是指机器学习模型在新数据上的性能往往比在训练数据上要差 

## 2.2神经网络的数据表示

**张量**是矩阵向任意维度的推广［注意，张量的**维度（dimension）**通常叫作**轴（axis）**］ 

### 2.2.1标量（0D 张量） 

1. 仅包含一个数字的张量叫作**标量**（scalar，也叫标量张量、零维张量、 0D 张量） 
2. 标量张量有 0 个轴（ndim == 0）。张量轴的个数也叫作**阶（rank） **

### 2.2.2向量（1D 张量） 

1. 数字组成的数组叫作**向量（vector）**或一维张量（1D 张量） 
2. 一维张量只有一个轴 

### 2.2.3矩阵（2D 张量） 

1. 向量组成的数组叫作**矩阵（matrix）**或二维张量（2D 张量） 
2. 矩阵有 2 个轴（通常叫作行和列） 

### 2.2.4 3D 张量与更高维张量 

1. 将多个矩阵组合成一个新的数组，可以得到一个 3D 张量，你可以将其直观地理解为数字组成的立方体
2. 将多个 3D 张量组合成一个数组，可以创建一个 4D 张量，以此类推。深度学习处理的一般是 0D 到 4D 的张量，但处理视频数据时可能会遇到 5D 张量 

### 2.2.5关键属性

1. 轴的个数（阶） 
2. 形状 
3. 数据类型 

## 2.3神经网络的“齿轮”：张量运算 

### 2.3.1逐元素运算

1. 逐元素（element-wise）的运算：该运算独立地应用于张量中的每个元素 
2. 运算非常适合大规模并行实现（向量化实现，这一术语来自于 1970—1990 年间向量处理器超级计算机架构） 

### 2.3.2广播 

1. 较小的张量会被广播（broadcast），以匹配较大张量的形状 
   - 向较小的张量添加轴（叫作广播轴），使其 ndim 与较大的张量相同 
   - 将较小的张量沿着新轴重复，使其形状与较大的张量相同 

### 2.3.3张量点积

点积运算，也叫张量积（tensor product，不要与逐元素的乘积弄混）

### 2.3.4张量变形 

1. 张量变形是指改变张量的行和列，以得到想要的形状 
2. 经常遇到的一种特殊的张量变形是转置（transposition） 

## 2.4神经网络的“引擎”：基于梯度的优化 

1. 训练循环：
   - 抽取训练样本 x 和对应目标 y 组成的数据批量
   - 在 x 上运行网络［这一步叫作**前向传播（forward pass）**］，得到预测值 y_pred 
   - 计算网络在这批数据上的损失，用于衡量 y_pred 和 y 之间的距离 
   - 更新网络的所有权重，使网络在这批数据上的损失略微下降 
2. 利用网络中所有运算都是**可微（differentiable）**的这一事实，计算损失相对于网络系数的**梯度（gradient）**，然后向梯度的反方向改变系数，从而使损失降低 

### 2.4.1什么是导数 

1. 导数
2. 可微

### 2.4.2张量运算的导数：梯度

1. 梯度（gradient）是张量运算的导数。它是导数这一概念向多元函数导数的推广 
2. 单变量函数 f(x) 的导数可以看作函数 f 曲线的斜率。同样，gradient(f)(W0) 也可以看作表示 f(W) 在 W0 附近**曲率（curvature）**的张量 
3. 对于张量的函数 f(W)，通过将 W 向梯度的反方向移动来减小 f(W) ，W1 = W0 - step * gradient(f)(W0)，其中 step 是一个很小的比例因子。比例因子 step 是必需的，因为gradient(f)(W0) 只是 W0 附近曲率的近似值，不能离 W0 太远 

### 2.4.3随机梯度下降 

1. **小批量随机梯度下降（mini-batch stochastic gradient descent，又称为小批量 SGD）**

   - 抽取训练样本 x 和对应目标 y 组成的数据批量 
   - 在 x 上运行网络，得到预测值 y_pred 
   - 计算网络在这批数据上的损失，用于衡量 y_pred 和 y 之间的距离 
   - 计算损失相对于网络参数的梯度［一次**反向传播（backward pass）**］ 
   - 将参数沿着梯度的反方向移动一点，比如 W -= step * gradient，从而使这批数据上的损失减小一点 

   **随机（stochastic）**是指每批数据都是随机抽取的 

2. **真 SGD** ：每次迭代时只抽取一个样本和目标，而不是抽取一批数据 

3. **批量 SGD**：每一次迭代都在所有数据上运行 

4. 神经网络的每一个权重参数都是空间中的一个自由维度，网络中可能包含数万个甚至上百万个参数维度 

5. SGD 还有多种变体，其区别在于计算下一次权重更新时还要考虑上一次权重更新，而不是仅仅考虑当前梯度值，比如带动量的 SGD、 Adagrad、 RMSProp 等变体。这些变体被称为**优化方法（optimization method）**或**优化器（optimizer）** 

6. **动量**解决了 SGD 的两个问题：收敛速度和局部极小点。将优化过程想象成一个小球从损失函数曲线上滚下来。如果小球的动量足够大，那么它不会卡在峡谷里，最终会到达全局最小点

   ```python
   past_velocity = 0. 
   momentum = 0.1 # 动量
   while loss > 0.01:
       w, loss, gradient = get_current_parameters()
       velocity = past_velocity * momentum - learning_rate * gradient
       w = w + momentum * velocity - learning_rate * gradient
       past_velocity = velocity
       update_parameter(w)
   ```

### 2.4.4链式求导：反向传播算法 

1. 链式法则
2. **反向传播**（backpropagation，有时也叫反式微分， reverse-mode differentiation）：反向传播从最终损失值开始，从最顶层反向作用至最底层，利用链式法则计算每个参数对损失值的贡献大小 
3. **符号微分**。给定一个运算链，并且已知每个运算的导数，框架（比如TensorFlow）就可以利用链式法则来计算这个运算链的**梯度函数**，将网络参数值映射为梯度值。对于这样的函数，反向传播就**简化为调用这个梯度函数 **

### 本章小结

- 学习是指找到一组模型参数，使得在给定的训练数据样本和对应目标值上的损失函数最小化 
- 学习的过程：随机选取包含数据样本及其目标值的批量，并计算批量损失相对于网络参数的梯度。随后将网络参数沿着梯度的反方向稍稍移动（移动距离由学习率指定） 
- 整个学习过程之所以能够实现，是因为神经网络是一系列可微分的张量运算，因此可以利用求导的链式法则来得到梯度函数，这个函数将当前参数和当前数据批量映射为一个梯度值
- 损失和优化器。将数据输入网络之前，需要先定义这二者 
- 损失是在训练过程中需要最小化的量，因此，它应该能够衡量当前任务是否已成功解决 
- 优化器是使用损失梯度更新参数的具体方式，比如 RMSProp 优化器、带动量的随机梯度下降（SGD）等 
